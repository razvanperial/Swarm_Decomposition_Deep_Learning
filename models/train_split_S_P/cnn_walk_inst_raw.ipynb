{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../../functions/get_data.ipynb\"\n",
    "%run \"../../functions/bandpass.ipynb\"\n",
    "%run \"../../functions/trimmer.ipynb\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../../data/swd_data/walk_inst_raw/\"\n",
    "old_folder_path = \"../../data/initial_data/walk_inst_raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/swd_data/walk_inst_raw/P_1.mat', '../../data/swd_data/walk_inst_raw/P_10.mat', '../../data/swd_data/walk_inst_raw/P_2.mat', '../../data/swd_data/walk_inst_raw/P_3.mat', '../../data/swd_data/walk_inst_raw/P_4.mat', '../../data/swd_data/walk_inst_raw/P_5.mat', '../../data/swd_data/walk_inst_raw/P_9.mat', '../../data/swd_data/walk_inst_raw/S_1.mat', '../../data/swd_data/walk_inst_raw/S_10.mat', '../../data/swd_data/walk_inst_raw/S_2.mat', '../../data/swd_data/walk_inst_raw/S_3.mat', '../../data/swd_data/walk_inst_raw/S_4.mat', '../../data/swd_data/walk_inst_raw/S_5.mat', '../../data/swd_data/walk_inst_raw/S_9.mat']\n",
      "['../../data/initial_data/walk_inst_raw/P_1.mat', '../../data/initial_data/walk_inst_raw/P_10.mat', '../../data/initial_data/walk_inst_raw/P_2.mat', '../../data/initial_data/walk_inst_raw/P_3.mat', '../../data/initial_data/walk_inst_raw/P_4.mat', '../../data/initial_data/walk_inst_raw/P_5.mat', '../../data/initial_data/walk_inst_raw/P_9.mat', '../../data/initial_data/walk_inst_raw/S_1.mat', '../../data/initial_data/walk_inst_raw/S_10.mat', '../../data/initial_data/walk_inst_raw/S_2.mat', '../../data/initial_data/walk_inst_raw/S_3.mat', '../../data/initial_data/walk_inst_raw/S_4.mat', '../../data/initial_data/walk_inst_raw/S_5.mat', '../../data/initial_data/walk_inst_raw/S_9.mat']\n"
     ]
    }
   ],
   "source": [
    "file_list = get_file_list(folder_path)\n",
    "old_file_list = get_file_list(old_folder_path)\n",
    "\n",
    "print(file_list)\n",
    "print(old_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 trials => 1 trials\n",
      "37 trials => 26 trials\n",
      "49 trials => 0 trials\n",
      "56 trials => 51 trials\n",
      "58 trials => 25 trials\n",
      "57 trials => 26 trials\n",
      "60 trials => 50 trials\n",
      "60 trials => 25 trials\n",
      "67 trials => 33 trials\n",
      "60 trials => 26 trials\n",
      "80 trials => 50 trials\n",
      "79 trials => 16 trials\n",
      "66 trials => 46 trials\n",
      "84 trials => 60 trials\n",
      "Total trials removed: 434 out of 869\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "removed_trials = 0\n",
    "initial_trials = 0\n",
    "\n",
    "for i, file in enumerate(file_list):\n",
    "    x = scipy.io.loadmat(file)\n",
    "    x = x['file_swds']\n",
    "\n",
    "    old_x = scipy.io.loadmat(old_file_list[i])\n",
    "    old_x = old_x['data']\n",
    "    initial_trials += old_x.shape[0]\n",
    "\n",
    "    if(x.shape[0] > 0):\n",
    "\n",
    "        print(f\"{old_x.shape[0]} trials => {x.shape[1]} trials\")\n",
    "\n",
    "        removed_trials = removed_trials + old_x.shape[0] - x.shape[1]\n",
    "        \n",
    "        data = np.array([ m[0] for m in x[0] ])\n",
    "        \n",
    "        first_letter = file.split('/')[-1][0]\n",
    "\n",
    "        if first_letter == 'P':\n",
    "            y = np.zeros(x.shape[1])\n",
    "        else:\n",
    "            y = np.ones(x.shape[1])\n",
    "            \n",
    "        Y.append(y)\n",
    "        X.append(data)\n",
    "\n",
    "    else:\n",
    "        print(f\"{old_x.shape[0]} trials => 0 trials\")\n",
    "        removed_trials += old_x.shape[0]\n",
    "\n",
    "print(f\"Total trials removed: {removed_trials} out of {initial_trials}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "for x in X:\n",
    "    data = []\n",
    "    for i, trial in enumerate(x):\n",
    "        trial_data = []\n",
    "        for j, channel in enumerate(trial):\n",
    "            channel_data = []\n",
    "            for k, ocm in enumerate(channel):\n",
    "                ocm_data = []\n",
    "                for l, sample in enumerate(ocm):\n",
    "                    ocm_data.append(sample)\n",
    "                channel_data.append(ocm_data)\n",
    "            trial_data.append(channel_data)\n",
    "        data.append(trial_data)\n",
    "\n",
    "    data = np.array(data)\n",
    "    input_data.append(data)\n",
    "\n",
    "\n",
    "# reshape the data in accordance to the input shape of the model\n",
    "for i, data in enumerate(input_data):\n",
    "    input_data[i] = np.reshape(data, (data.shape[0], data.shape[1], data.shape[3], data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11, 250, 3) (1,)\n",
      "(26, 11, 250, 3) (26,)\n",
      "(51, 11, 250, 3) (51,)\n",
      "(25, 11, 250, 3) (25,)\n",
      "(26, 11, 250, 3) (26,)\n",
      "(50, 11, 250, 3) (50,)\n",
      "(25, 11, 250, 3) (25,)\n",
      "(33, 11, 250, 3) (33,)\n",
      "(26, 11, 250, 3) (26,)\n",
      "(50, 11, 250, 3) (50,)\n",
      "(16, 11, 250, 3) (16,)\n",
      "(46, 11, 250, 3) (46,)\n",
      "(60, 11, 250, 3) (60,)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(input_data):\n",
    "    print(data.shape, Y[i].shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINING, COMPILING AND TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "best_model = None\n",
    "\n",
    "accuracies = []\n",
    "p_acc = []\n",
    "s_acc = []\n",
    "\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "for index_1 in [1,2,3,4,5]:\n",
    "    for index_2 in range(6, 13):\n",
    "\n",
    "        print(f\"Training on {index_1} and {index_2}\")\n",
    "\n",
    "        # build training set from allfiles expect the ones with index_1 and index_2\n",
    "        X_train = [input_data[i] for i in range(len(input_data)) if i != index_1 and i != index_2]\n",
    "        Y_train = [Y[i] for i in range(len(Y)) if i != index_1 and i != index_2]\n",
    "\n",
    "        # build test set from the files with index_1 and index_2\n",
    "        X_test = [input_data[i] for i in range(len(input_data)) if i == index_1 or i == index_2]\n",
    "        Y_test = [Y[i] for i in range(len(Y)) if i == index_1 or i == index_2]\n",
    "\n",
    "        X_train = np.concatenate(X_train)\n",
    "        Y_train = np.concatenate(Y_train)\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(30, (8,8), activation='relu', input_shape=(11, 250, 3)),\n",
    "            tf.keras.layers.MaxPooling2D((2,2)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(2, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, Y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "        # make prediction on the 2 test files individually\n",
    "        X_test_1 = X_test[0]\n",
    "        Y_test_1 = Y_test[0]\n",
    "        X_test_2 = X_test[1]\n",
    "        Y_test_2 = Y_test[1]\n",
    "\n",
    "        probs_1 = model.predict(X_test_1)\n",
    "        probs_2 = model.predict(X_test_2)\n",
    "\n",
    "        # get accuracy for each test file\n",
    "        acc_1 = balanced_accuracy_score(Y_test_1, probs_1.round())\n",
    "        acc_2 = balanced_accuracy_score(Y_test_2, probs_2.round())\n",
    "\n",
    "        p_acc.append(acc_1)\n",
    "        s_acc.append(acc_2)\n",
    "\n",
    "        # get average accuracy\n",
    "        acc = (acc_1 + acc_2) / 2\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        if(acc > best_acc):\n",
    "            best_acc = acc\n",
    "            best_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.658695652173913\n",
      "Average accuracy: 0.5323070550506612\n",
      "Std accuracy: 0.07511040185459088\n",
      "Average accuracy for P: 0.2276181857358328 ± 0.18869231316235932\n",
      "Average accuracy for S: 0.8369959243654895 ± 0.141028839518155\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best accuracy: {best_acc}\")\n",
    "print(f\"Average accuracy: {np.mean(accuracies)} ± {np.std(accuracies)}\")\n",
    "\n",
    "print(f\"Average accuracy for P: {np.mean(p_acc)} ± {np.std(p_acc)}\")\n",
    "print(f\"Average accuracy for S: {np.mean(s_acc)} ± {np.std(s_acc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_net_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
